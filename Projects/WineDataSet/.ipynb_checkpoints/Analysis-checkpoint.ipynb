{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n",
    "\n",
    "# check which datasets can be imported\n",
    "#list_available_datasets()\n",
    "\n",
    "# import dataset\n",
    "wine_dataset = fetch_ucirepo(id=186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b287fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wine_dataset.data.targets\n",
    "X = wine_dataset.data.features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(wine_dataset.metadata.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36915036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(wine_dataset.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff93b6",
   "metadata": {},
   "source": [
    "Note:\n",
    "This dataset has an additional column where you can see if the wine was red or white, could be a great binary classification problem.  Right now we will focus on predicting the quality of the wine from 0-10 which makes this a multi-classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def84f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing values\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of full dataset:{X.shape}')\n",
    "print(f'Size of training dataset:{X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_full = pd.concat([X_train,y_train],axis=1)\n",
    "X_full.groupby('quality').describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aaff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc2afe",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- The dataset is imbalanced, with the majority of the data being of quality 5 and 6.\n",
    "- The dataset has no missing values.\n",
    "- The dataset has 11 features and 1 target variable.\n",
    "- Fitting a model to this dataset would not predict the quality of the wine well, as the dataset is imbalanced.The model would predict the majority class most of the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets look at the distribution of features per class\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=X_full, x='quality', y='fixed_acidity',\n",
    "            showmeans=True, meanprops={\"marker\":\"o\",\n",
    "                                        \"markerfacecolor\":\"white\", \n",
    "                                        \"markeredgecolor\":\"black\"},\n",
    "                                        showfliers=False, palette='viridis')\n",
    "plt.title('Fixed Acidity vs Quality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b193784",
   "metadata": {},
   "source": [
    "No outliershow means are inflated because plot excludes outliers but uses them to calculate the means in the data.\n",
    "Wine acicity decreases with quality of the wine,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce29b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=2, figsize=(10,20))\n",
    "i = 0;j = 0\n",
    "for feature in X_full.columns[:-1]:\n",
    "    sns.boxplot(data=X_full,x='quality', y=feature,\n",
    "                showmeans=True, meanprops={\"marker\":\"o\",\n",
    "                                            \"markerfacecolor\":\"white\", \n",
    "                                            \"markeredgecolor\":\"black\"},\n",
    "                                            showfliers=False, palette='viridis',\n",
    "                                            ax=axes[i,j])\n",
    "    axes[i,j].set_title(f'{feature} vs Quality')\n",
    "    \n",
    "    if i < 5:\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1\n",
    "        i = 0\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sns.heatmap(X_train.corr(), annot = True, mask = np.triu(X.corr()), \n",
    "            cmap='viridis', fmt='.2f')            \n",
    "plt.title('Pearson Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15677fa4",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- PH, residual_sugar, sulphates and total sulfur dioxide seems unaffected by quality based on mean, although there are variable variance mostly due to variation in sample sizes per category.\n",
    "- Correlation between free_sulfur dioxide and total_sulfur which may indicate that total sulfur is a measurement that might involve in itself the addition of free_sulfur (redundant variable).\n",
    "- Negative correlation between density and alcohol \"strong\" after quality 5 and above. No strong relationship found in other variables. \n",
    "- These findings would be similar to what would be found under an ANOVA test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69dd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('free_sulfur_dioxide', axis=1, inplace=True)\n",
    "X_test.drop('free_sulfur_dioxide', axis=1, inplace=True)\n",
    "\n",
    "## Distribution of features\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,20))\n",
    "i = 0;j = 0\n",
    "for feature in X_train.columns:\n",
    "    sns.histplot(data=X_train,x=feature, kde=True, ax=axes[i,j], color='blue')\n",
    "    axes[i,j].set_title(f'{feature} Distribution')\n",
    "    axes[i,j].set_xlabel('') #remove x label\n",
    "\n",
    "    #Update axes\n",
    "    if i < 4:\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec857f30",
   "metadata": {},
   "source": [
    "Some features have a right skew so lets apply a log transformation to shift its distribution to a more normal one.  \n",
    "Then I will apply a standarization,  before doing the downstream analysis I would like to do feature selection and move forward with the best features for the model, this to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ce27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Before transformation of features I will create a copy of the original dataset\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "\n",
    "features_to_scale = ['volatile_acidity','chlorides','residual_sugar','density','pH','sulphates','alcohol']\n",
    "for f in features_to_scale:\n",
    "    X_train['log_'+f] = np.log(X_train[f])\n",
    "    X_test['log_'+f] = np.log(X_test[f])\n",
    "\n",
    "X_train.drop(features_to_scale, axis=1, inplace=True)\n",
    "X_test.drop(features_to_scale, axis=1, inplace=True)    \n",
    "\n",
    "## Distribution of features\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(10,20))\n",
    "i = 0;j = 0\n",
    "for feature in X_train.columns:\n",
    "    sns.histplot(data=X_train, x=feature, kde=True, ax=axes[i,j], color='blue')\n",
    "    axes[i,j].set_title(f'{feature} Distribution')\n",
    "    axes[i,j].set_xlabel('') #remove x label\n",
    "\n",
    "    #Update axes\n",
    "    if i < 4:\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1\n",
    "        i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d57d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sns.heatmap(X_train.corr(), annot = True, mask = np.triu(X_train.corr()), \n",
    "            cmap='viridis', fmt='.2f')            \n",
    "plt.title('Vars Pearson Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection\n",
    "# LETS standarize the data prior to feature selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataset\n",
    "import os\n",
    "os.makedirs('Data', exist_ok=True)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_train_scaled['quality'] = y_train\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "X_test_scaled['quality'] = y_test\n",
    "X_train_scaled.to_csv('Data/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('Data/X_test_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini_importance\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33def390",
   "metadata": {},
   "outputs": [],
   "source": [
    "top4 = X_train.columns[indices][:4]\n",
    "top4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Impurity based importance\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b4988",
   "metadata": {},
   "source": [
    "before training... I think that re-classifying the target variable is a good idea, due to the imbalance of the classes. I will classify the wines as good or bad based on the quality. \n",
    "- Wines with a quality of 3-4 will be classified as quality low\n",
    "- Wines with a quality of 5-6 will be classified as quality medium\n",
    "- Wines with a quality of 7-9 will be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d572a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)\n",
    "y_train = y_train.astype('str')\n",
    "y_test = y_test.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(np.array(y_train_mapped).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets now deal with the imbalanced dataset using SMOTE -- Synthetic Mibirotic Over Sampling Technique\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "oversample = SMOTE(k_neighbors=2)\n",
    "\n",
    "X_resampled, y_train_resampled = oversample.fit_resample(X_train[top4], y_train)\n",
    "\n",
    "print(f'Original dataset shape {y_train.value_counts()}')\n",
    "print(f'Resampled dataset shape {y_train_resampled.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7a4c7",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "Try:\n",
    "1. Multi-class AdaBoost\n",
    "2. CatBoost\n",
    "3. XGBoost\n",
    "4. LightGBM\n",
    "5. Random Forest\n",
    "6. SVM\n",
    "7. Neural Network, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8652d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit models on the resampled dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "remap_dict = {'3':'low',\n",
    "            '4':'low', \n",
    "            '5':'medium',\n",
    "            '6':'medium',\n",
    "            '7':'medium',\n",
    "            '8':'high',\n",
    "            '9':'high'}\n",
    "\n",
    "y_train_mapped = y_train_resampled.replace(remap_dict)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "y_train_encoded = encoder.fit_transform(y_train_mapped.values)\n",
    "y_test_encoded = encoder.fit_transform(y_test.replace(remap_dict))\n",
    "\n",
    "for n in [50,100,200]:\n",
    "    clf = RandomForestClassifier(n_estimators=n, random_state=0, n_jobs=-1, class_weight='balanced')\n",
    "    clf.fit(X_resampled, y_train_encoded)\n",
    "    y_pred = clf.predict(X_test[top4])\n",
    "\n",
    "    print(f'Classification report for {n} estimators')\n",
    "    print(classification_report(y_test_encoded, y_pred))\n",
    "    print((f'Accuracy score for {n} estimators: %s') % (accuracy_score(y_test_encoded, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a09d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-Layer Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "for activation in ['tanh', 'relu','logistic']:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=0,\n",
    "                        solver='adam', activation=activation)\n",
    "    clf.fit(X_resampled, y_train_encoded)\n",
    "    y_predNN = clf.predict(X_test[top4])\n",
    "\n",
    "    print(f\"Accuracy with activation: {activation}  is {round(accuracy_score(y_test_encoded, y_predNN))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba808d5e",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "- CatBoost is a gradient boosting library that is designed for handling categorical data. It is based on decision trees and is designed to work with categorical data. It is a powerful library that can handle categorical data without the need for one-hot encoding.\n",
    "- Reduces time in parameter tunning \n",
    "- Reduction of overfitting with gradient boosting on decision trees.\n",
    "- Incorporation of various types of data, doesnt need to be from a homogeneous source\n",
    "- Training data limited whereas deep learning requires a lot of data.\n",
    " - **boosting methods sequentially learn from weak classifiers  with the goal of minimizing errors from preceding classifiers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a174df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bcd87e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## CatBoost\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[1;32m      3\u001b[0m CatModel \u001b[38;5;241m=\u001b[39m CatBoostClassifier()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "## CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "CatModel = CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd9f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
